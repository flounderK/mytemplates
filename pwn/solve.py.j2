#!/usr/bin/env python3
from pwn import *
import monkeyhex
import time
import argparse
import re
import os
from functools import partial
import logging

# Run with ipython3 -i solve.py -- DEBUG
{% if ONE_GADGET is defined -%}
# Run with ipython3 -i solve.py -- DEBUG <one_gadget>
parser = argparse.ArgumentParser()
parser.add_argument("one_gadget", type=partial(int, base=0), nargs=argparse.REMAINDER)
argparse_args = parser.parse_args()
{%- endif %}
# context.log_level = 'debug'
context.terminal = ['gnome-terminal', '-e']

_CACHED_LIBC_PATH = None
def get_preloadable_libc(path=None, libc_paths=[]):
    """
    Recursively search the working directory for libc
    """
    global _CACHED_LIBC_PATH
    if _CACHED_LIBC_PATH is not None:
        return _CACHED_LIBC_PATH
    if path is None:
        path = os.getcwd()
    for root, dirs, files in os.walk(path):
        for f in files:
            # match common libc-2.31.so and libc.so.6 formats
            match = re.search(r'libc(\.so\.6|-\d+\.\d+\.so)', f)
            if match is not None:
                libc_paths.append(os.path.join(root, f))

    if len(libc_paths) > 0:
        return libc_paths[0]
    return None

# this variable will be filled in with an `ELF` object if
# there is a libc in the same directory as (or child directories of) the script
libc = None
script_directory = os.path.dirname(os.path.abspath(__file__))
LIBC_PATH = get_preloadable_libc(path=script_directory)
if libc is None and LIBC_PATH is not None:
    libc = ELF(LIBC_PATH)
    binsh_offset = libc.data.find(b'/bin/sh')
    if binsh_offset != -1:
        libc.sym['binsh'] = libc.offset_to_vaddr(binsh_offset)

    # libc.sym['one_gadget'] = argparse_args.one_gadget[0] if argparse_args.one_gadget else 0
# libc = ELF('/lib/x86_64-linux-gnu/libc.so.6') if not args.REMOTE else ELF('{{ libc_path|d('libc.so.6', true) }}')
binary = context.binary = ELF('{{ binary_path }}')


def attach_gdb(p, commands=None):
    """Template to run gdb with predefined commands on a process."""
    val = """
    c
    """ if commands is None else commands
    res = gdb.attach(p, val)
    pause()
    return res


def new_proc(start_gdb=False, gdbscript=None, force_unbuffered=False,
             skip_libc_preload=False,
             preload_libs=None, ld_libpath_cwd=False):
    """
    Start a new process with predefined debug operations.
    @start_gdb: start gdb on process start up
    @gdbscript: default gdbscript to use
    @force_unbuffered: force input to be unbuffered. Use this if outputs arent
                       getting back to you but the program is sending them,
                       or if there is just general input/output weirdness
    @skip_libc_preload: By default this script will attempt to LD_PRELOAD any
                        libc.so.6 (or other libc). If this behavior is not
                        what you want, you can disable it by setting this
                        to True
    @preload_libs: If you would like to preload additional libraries, pass a
                   list of their paths here
   @ld_libpath_cwd: Often, binaries are meant for a different OS version or
                    different distribution of linux entirely from your current
                    one. If you wish to run them without setting up a docker
                    container you can do so by extracting the desired libraries
                    from a docker container (`get_docker_libs.sh` helps)
                    then patching your binary with
                    patchelf --set-interpreter "<ld-interpreter>" "<binary>"
                    and setting this variable. This is exceptionally overkill,
                    but prevents you from having to debug/test through a docker
                    container, which is a bit of a pain
    """
    kwargs = {}
    kwargs["env"] = {}
    # if there is a libc in the current directory
    global LIBC_PATH
    if skip_libc_preload is False:
        if LIBC_PATH is not None:
            if preload_libs:
                preload_libs.append(LIBC_PATH)
            else:
                preload_libs = [LIBC_PATH]
    if preload_libs:
        cwd = os.getcwd()
        preload_libs = [os.path.join(cwd, i) if not i.startswith("/") else i
                        for i in preload_libs]
        ld_preload = kwargs['env'].get('LD_PRELOAD')
        if ld_preload:
            ld_preload = ld_preload.split(" ")
        else:
            ld_preload = []
        ld_preload.extend(preload_libs)
        kwargs['env']['LD_PRELOAD'] = " ".join(ld_preload)

    if ld_libpath_cwd:
        kwargs['env']['LD_LIBRARY_PATH'] = os.getcwd()

    if force_unbuffered is True:
        kwargs['stdin'] = process.PTY
        kwargs['stdout'] = process.PTY

    p = process(binary.path, **kwargs)
    if start_gdb is True:
        attach_gdb(p, gdbscript)
    return p

{% if HEAP_UTILS is defined -%}
def uniquish_char():
    """Track order of arbitrary inputs in a more systematic way.
    use like: CHAR_GEN = uniquish_char()
    p.send(next(CHAR_GEN)*10)
    """
    for i in string.ascii_letters:
        yield i.encode()

CHAR_GEN = uniquish_char()

def bnot(n, numbits=context.bits):
    return (1 << numbits) -1 -n

def align(val, align_to, numbits=context.bits):
    return val & bnot(align_to - 1, numbits)

def align_up(val, align_to, numbits=context.bits):
    aligned = align(val, align_to, numbits)
    if aligned < val:
        aligned += align_to
    return aligned

def batch(it, sz):
    length = len(it)
    for i in range(0, length, sz):
        yield it[i:i+sz]

def group_by_increment(iterable, group_incr):
    """
    Identify series of values that increment/decrement
    by the same amount, grouping them into lists.
    Useful for finding heap chunks next to eachother
    from large leaks
    """
    grouped = []
    current = [iterable[0]]
    for i in range(1, len(iterable)):
        curr_val = iterable[i]
        prev_val = current[-1]
        if (prev_val + group_incr) == curr_val:
            current.append(curr_val)
        else:
            grouped.append(current)
            current = [curr_val]
    if current:
        grouped.append(current)
    return grouped

SIZE_SZ = context.bytes
MALLOC_ALIGNMENT = SIZE_SZ*2
MALLOC_ALIGN_MASK = MALLOC_ALIGNMENT - 1
MIN_CHUNK_SIZE = SIZE_SZ*4
MINSIZE = (MIN_CHUNK_SIZE+MALLOC_ALIGN_MASK) & ~MALLOC_ALIGN_MASK
def calc_chunksize(x):
    if (x + SIZE_SZ + MALLOC_ALIGN_MASK) < MINSIZE:
        chunksize = MINSIZE
    else:
        chunksize = (x + SIZE_SZ + MALLOC_ALIGN_MASK) & ~MALLOC_ALIGN_MASK
    return chunksize

def calc_tcache_idx(x):
    CHUNKSIZE = calc_chunksize(x)
    IDX = (CHUNKSIZE - MINSIZE + MALLOC_ALIGNMENT - 1) // MALLOC_ALIGNMENT
    return IDX

def reveal_ptr(ptr_addr: int, ptr: int) -> int:
    """unmask heap pointer after glibc 2.32"""
    return ptr_addr >> 12 ^ ptr
{%- endif %}
{% if BOF_CHECK is defined -%}
def bof_check():
    """Check a local process for easy to detect
    buffer overflow"""
    if hasattr(p, "proc") is False:
        return 0, None
    if p.poll() is None or p.poll() >= 0:
        return 0, None

    c = p.corefile
    log.success("Fault addr %#x", c.fault_addr)
    rsp_offset = c.sp - c.stack.address
    data_at_rsp_offset = c.stack.data[rsp_offset:rsp_offset+context.bytes]
    code_near_pc = c.disasm(c.pc, 16)
    on_ret_instr = False
    if len(code_near_pc) > 0:
        current_instr_ln = code_near_pc.splitlines()[0]
        on_ret_instr = bool(re.search('(?i)RET', current_instr_ln))
    log.success("data at rsp %s", data_at_rsp_offset)
    offset = cyclic_find(data_at_rsp_offset[:4])
    if (offset != -1 or c.fault_addr == data_at_rsp_offset) \
       and on_ret_instr is True:
        log.success("Looks like the next return address was overwritten!")
        log.success("offset %#x", offset)
        # for i in c.registers.items():
        #     log.success("%s: %#x" % (i))
    return offset, c

{%- endif %}
p = new_proc(context.log_level == logging.DEBUG) if not args.REMOTE else remote('{{ remote_host|d("localhost", true) }}', {{ port|d(8000, true) }})
# do leak / payload gen here

payload = b''
{% if JOP is defined -%}
# just to track which gadgets are actually being used
used_dt_gadgets = set()

# table of gadget addresses
dispatch_table = [
    b'/bin/sh\x00',  # only allowed because is 8 bytes
    # dispatch gadget here

]

# start of controlled input (needs leak)
stack_loc = 0

# index where the dispatch table may not extend past
dispatch_table_limit = 0xb0
# offset of dispatch table from start of input
dispatch_table_offset = dispatch_table_limit - (len(dispatch_table)*8)
# dispatch_table_addr on stack
dta = stack_loc + dispatch_table_offset


def dta_for(gadget):
    """Just to make payload more readable,
    because actual instr has to be an address that contains
    the address of a gadget ('jmp rdx' vs 'jmp [rdx]')"""
    # convert gadget symbol name to address
    if isinstance(gadget, str):
        gadget = binary.sym[gadget]

    used_dt_gadgets.add(gadget)
    return dta + (dispatch_table.index(gadget)*8)


payload = flat({
    # dispatch table
    dispatch_table_offset: dispatch_table
})
{%- endif %}
# p.send(cyclic(0x200) + b'\n')
# p.interactive()
